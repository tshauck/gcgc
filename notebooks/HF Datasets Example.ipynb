{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a demo of how to use GCGC to create a huggingface dataset object from a uniprot dataset.\n",
    "\n",
    "Install gcgc with support for the hugging face package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ""
     ]
    }
   ],
   "source": [
    "!pip install 'gcgc[hf]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will use swissprot because it's the smallest, and easiest to manage. However the common uniprot datasets are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uniref50, uniref90, uniref100, uniparc, trembl, sprot'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gcgc.third_party import hf_datasets\n",
    "\n",
    "\", \".join(hf_datasets.UniprotDatasetNames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create the dataset reference. This is responsible for downloading the file and preparing it for the datasets package, which for gcgc means making the request to uniprot, then parsing the resultant FASTA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset uniprot_dataset (/Users/thauck/.cache/huggingface/datasets/uniprot_dataset/sprot/1.0.0)\n"
     ]
    }
   ],
   "source": [
    "ref = hf_datasets.UniprotDataset(name=\"sprot\")\n",
    "\n",
    "# This will be a noop, if the dataset is cached.\n",
    "ref.download_and_prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then it's possible to extract the specific split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ref.as_dataset(\"sprot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.arrow_dataset.Dataset"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the sequences are in the nice arrow based dataset, let's have a peak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL\n",
      "MSIIGATRLQNDKSDTYSAGPCYAGGCSAFTPRGTCGKDWDLGEQTCASGFCTSQPLCARIKKTQVCGLRYSSKGKDPLVSAEWDSRGAPYVRCTYDADLIDTQAQVDQFVSMFGESPSLAERYCMRGVKNTAGELVSRVSSDADPAGGWCRKWYSAHRGPDQDAALGSFCIKNPGAADCKCINRASDPVYQKVKTLHAYPDQCWYVPCAADVGELKMGTQRDTPTNCPTQVCQIVFNMLDDGSVTMDDVKNTINCDFSKYVPPPPPPKPTPPTPPTPPTPPTPPTPPTPPTPRPVHNRKVMFFVAGAVLVAILISTVRW\n",
      "MASNTVSAQGGSNRPVRDFSNIQDVAQFLLFDPIWNEQPGSIVPWKMNREQALAERYPELQTSEPSEDYSGPVESLELLPLEIKLDIMQYLSWEQISWCKHPWLWTRWYKDNVVRVSAITFEDFQREYAFPEKIQEIHFTDTRAEEIKAILETTPNVTRLVIRRIDDMNYNTHGDLGLDDLEFLTHLMVEDACGFTDFWAPSLTHLTIKNLDMHPRWFGPVMDGIKSMQSTLKYLYIFETYGVNKPFVQWCTDNIETFYCTNSYRYENVPRPIYVWVLFQEDEWHGYRVEDNKFHRRYMYSTILHKRDTDWVENNPLKTPAQVEMYKFLLRISQLNRDGTGYESDSDPENEHFDDESFSSGEEDSSDEDDPTWAPDSDDSDWETETEEEPSVAARILEKGKLTITNLMKSLGFKPKPKKIQSIDRYFCSLDSNYNSEDEDFEYDSDSEDDDSDSEDDC\n",
      "MYQAINPCPQSWYGSPQLEREIVCKMSGAPHYPNYYPVHPNALGGAWFDTSLNARSLTTTPSLTTCTPPSLAACTPPTSLGMVDSPPHINPPRRIGTLCFDFGSAKSPQRCECVASDRPSTTSNTAPDTYRLLITNSKTRKNNYGTCRLEPLTYGI\n",
      "MARPLLGKTSSVRRRLESLSACSIFFFLRKFCQKMASLVFLNSPVYQMSNILLTERRQVDRAMGGSDDDGVMVVALSPSDFKTVLGSALLAVERDMVHVVPKYLQTPGILHDMLVLLTPIFGEALSVDMSGATDVMVQQIATAGFVDVDPLHSSVSWKDNVSCPVALLAVSNAVRTMMGQPCQVTLIIDVGTQNILRDLVNLPVEMSGDLQVMAYTKDPLGKVPAVGVSVFDSGSVQKGDAHSVGAPDGLVSFHTHPVSSAVELNYHAGWPSNVDMSSLLTMKNLMHVVVAEEGLWTMARTLSMQRLTKVLTDAEKDVMRAAAFNLFLPLNELRVMGTKDSNNKSLKTYFEVFETFTIGALMKHSGVTPTAFVDRRWLDNTIYHMGFIPWGRDMRFVVEYDLDGTNPFLNTVPTLMSVKRKAKIQEMFDNMVSRMVTS\n"
     ]
    }
   ],
   "source": [
    "five_records = ds[:5]\n",
    "print(\"\\n\".join(five_records['sequence']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks likes some proteins... curious what's up with the proline rich area in the second sequence??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are amino acid strings. For modeling, it needs to go through some process to tokenize the sequences, which for gcgc means using its KmerTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gcgc import KmerTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is using an extended protein alphabet that will conform the underlying sequence length to 200 (trim or pad as necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = KmerTokenizer(alphabet=\"extended_protein\", conform_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KmerTokenizer(vocab=Vocab(stoi={'|': 0, '>': 1, '<': 2, '#': 3, '?': 4, 'A': 5, 'C': 6, 'D': 7, 'E': 8, 'F': 9, 'G': 10, 'H': 11, 'I': 12, 'K': 13, 'L': 14, 'M': 15, 'N': 16, 'P': 17, 'Q': 18, 'R': 19, 'S': 20, 'T': 21, 'V': 22, 'W': 23, 'Y': 24, 'B': 25, 'X': 26, 'Z': 27, 'J': 28, 'U': 29, 'O': 30}), pad_token='|', bos_token='>', eos_token='<', mask_token='#', unk_token='?', pad_token_id=0, bos_token_id=1, eos_token_id=2, mask_token_id=3, unk_token_id=4, pad_at_end=True, max_length=None, min_length=None, conform_length=200, alphabet='ACDEFGHIKLMNPQRSTVWYBXZJUO', kmer_length=1, kmer_stride=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can map this over the dataset to create a new dataset with an `input_ids` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/thauck/.cache/huggingface/datasets/uniprot_dataset/sprot/1.0.0/cache-c9c4b3bc9d2ef833.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = ds.map(lambda x: {\"input_ids\": tokenizer.encode(x[\"sequence\"])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset(features: {'description': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'sequence': Value(dtype='string', id=None)}, num_rows: 563082)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the new input_ids feature\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at a row, things look good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'sp|Q6GZX4|001R_FRG3G Putative transcription factor 001R OS=Frog virus 3 (isolate Goorha) OX=654924 GN=FV3-001R PE=4 SV=1', 'id': 'sp|Q6GZX4|001R_FRG3G', 'input_ids': [1, 15, 5, 9, 20, 5, 8, 7, 22, 14, 13, 8, 24, 7, 19, 19, 19, 19, 15, 8, 5, 14, 14, 14, 20, 14, 24, 24, 17, 16, 7, 19, 13, 14, 14, 7, 24, 13, 8, 23, 20, 17, 17, 19, 22, 18, 22, 8, 6, 17, 13, 5, 17, 22, 8, 23, 16, 16, 17, 17, 20, 8, 13, 10, 14, 12, 22, 10, 11, 9, 20, 10, 12, 13, 24, 13, 10, 8, 13, 5, 18, 5, 20, 8, 22, 7, 22, 16, 13, 15, 6, 6, 23, 22, 20, 13, 9, 13, 7, 5, 15, 19, 19, 24, 18, 10, 12, 18, 21, 6, 13, 12, 17, 10, 13, 22, 14, 20, 7, 14, 7, 5, 13, 12, 13, 5, 24, 16, 14, 21, 22, 8, 10, 22, 8, 10, 9, 22, 19, 24, 20, 19, 22, 21, 13, 18, 11, 22, 5, 5, 9, 14, 13, 8, 14, 19, 11, 20, 13, 18, 24, 8, 16, 22, 16, 14, 12, 11, 24, 12, 14, 21, 7, 13, 19, 22, 7, 12, 18, 11, 14, 8, 13, 7, 14, 22, 13, 7, 9, 13, 5, 14, 22, 8, 20, 5, 11, 19, 15, 19], 'sequence': 'MAFSAEDVLKEYDRRRRMEALLLSLYYPNDRKLLDYKEWSPPRVQVECPKAPVEWNNPPSEKGLIVGHFSGIKYKGEKAQASEVDVNKMCCWVSKFKDAMRRYQGIQTCKIPGKVLSDLDAKIKAYNLTVEGVEGFVRYSRVTKQHVAAFLKELRHSKQYENVNLIHYILTDKRVDIQHLEKDLVKDFKALVESAHRMRQGHMINVKYILYQLLKKHGHGPDGPDILTVKTGSKGVLYDDSFRKIYTDLGWKFTPL'}\n"
     ]
    }
   ],
   "source": [
    "for x in encoded_dataset:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this is not a particularly fast process, but caching thanks to hugging face helps alot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/thauck/.cache/huggingface/datasets/uniprot_dataset/sprot/1.0.0/cache-c9c4b3bc9d2ef833.arrow\n"
     ]
    }
   ],
   "source": [
    "encoded_dataset = ds.map(lambda x: {\"input_ids\": tokenizer.encode(x[\"sequence\"])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's also a batch mode, which gives some speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/thauck/.cache/huggingface/datasets/uniprot_dataset/sprot/1.0.0/cache-289f655b6eeb11a7.arrow\n"
     ]
    }
   ],
   "source": [
    "second_ds = ds.map(lambda x: {\"input_ids\": tokenizer.encode_batch(x[\"sequence\"])}, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyways, so we have our tokenized dataet, and we're ready to do some (data) science."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('gcgc': conda)",
   "language": "python",
   "name": "python37764bitgcgccondaa16d650929c34fd1a38ca7dadf02a617"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
